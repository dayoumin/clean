# app.py

import os
import sys
import openai
import logging
import warnings
import json
import re
import traceback
from pathlib import Path

import faiss
import streamlit as st
from dotenv import load_dotenv
import tiktoken

from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
from langchain.docstore.document import Document

# =========================
# 1. CONFIG ë”•ì…”ë„ˆë¦¬ ì •ì˜
# =========================

CONFIG = {
    "model_name": "gpt-4o",  # LLM ëª¨ë¸ëª… (ì˜¤íƒ€ ìˆ˜ì •: "gpt-4o" â†’ "gpt-4")
    "embedding_model": "text-embedding-3-large",  # ì„ë² ë”© ëª¨ë¸ëª… (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •)
    "vectorstore_path": "faiss_vectorstore",  # ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    "hash_file_path": "faiss_vectorstore/document_hashes.json",  # ë¬¸ì„œ í•´ì‹œ íŒŒì¼ ê²½ë¡œ
    "api_keys": {
        "openai": None,  # OpenAI API í‚¤
    }
}

# =========================
# 2. ë¡œê¹… ì„¤ì •
# =========================

class CustomFormatter(logging.Formatter):
    def format(self, record):
        return f"\n--- {record.levelname} ---\n{super().format(record)}\n"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('chat_server.log', encoding='utf-8')
    ]
)

for handler in logging.root.handlers:
    handler.setFormatter(CustomFormatter())

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=DeprecationWarning)

# =========================
# 3. API í‚¤ ë¡œë“œ
# =========================

load_dotenv()

CONFIG["api_keys"]["openai"] = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not CONFIG["api_keys"]["openai"]:
    logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

openai.api_key = CONFIG["api_keys"]["openai"]

# =========================
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# =========================

def extract_law_name(file_name):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ë²•ë¥ ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    name = Path(file_name).stem
    name = re.sub(r'[_\-]', ' ', name)
    return name

def load_document_hashes(hash_file_path: Path) -> dict:
    """ì €ì¥ëœ ë¬¸ì„œ í•´ì‹œ ë¡œë“œ"""
    try:
        return json.loads(hash_file_path.read_text())
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

# =========================
# 5. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í•¨ìˆ˜
# =========================

def initialize_vectorstore(embedding_model):
    VECTORSTORE_PATH = Path(CONFIG["vectorstore_path"])
    HASH_FILE_PATH = Path(CONFIG["hash_file_path"])

    embeddings = OpenAIEmbeddings(model=embedding_model, openai_api_key=CONFIG["api_keys"]["openai"])

    if 'vectorstore' in st.session_state:
        logger.info("ì„¸ì…˜ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state['vectorstore'], HASH_FILE_PATH

    try:
        vectorstore = LangChainFAISS.load_local(
            str(VECTORSTORE_PATH), 
            embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    st.session_state['vectorstore'] = vectorstore
    return vectorstore, HASH_FILE_PATH

# =========================
# 6. ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ì •ì˜
# =========================

class StreamlitCallbackHandler(BaseCallbackHandler):
    """Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ í‘œì‹œí•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬."""

    def __init__(self, message_placeholder):
        self.message_placeholder = message_placeholder
        self.answer_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        skip_patterns = ["Human:", "Assistant:", "ì§ˆë¬¸: ", "ë‹µë³€: "]
        if any(pattern in token for pattern in skip_patterns):
            return

        self.answer_text += token
        self.message_placeholder.markdown(self.answer_text + "â–Œ")

    def on_llm_end(self, response, **kwargs):
        """LLM ì‘ë‹µ ì™„ë£Œë˜ë©´ í˜¸ì¶œë©ë‹ˆë‹¤."""
        final_answer = self.answer_text.strip()
        self.message_placeholder.markdown(final_answer)
        self.answer_text = ""

# =========================
# 7. í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
# =========================

def log_document_tokens(docs):
    """
    ê° ë¬¸ì„œì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    total_tokens = 0
    for i, doc in enumerate(docs, 1):
        tokens = len(tokenizer.encode(doc.page_content))
        total_tokens += tokens
        logger.debug(f"[ë¬¸ì„œ {i}] í† í° ìˆ˜: {tokens}")

    logger.debug(f"ì´ ë¬¸ì„œ í† í° ìˆ˜: {total_tokens}")
    return total_tokens

# =========================
# 8. ChatPromptTemplate ì •ì˜
# =========================

chat_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """ë‹¹ì‹ ì€ í•œêµ­ì˜ ìµœê³  ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. [ì¤‘ìš”] ë¨¼ì € ì§ˆë¬¸ì˜ ë‚´ìš©ì´ ì¸ì‚¿ë§ì¸ì§€ ë²•ë¥ ì ì¸ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”. ë²•ë¥ ì ì¸ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš° ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”. 
   ë²•ë¥ ì ì¸ ì§ˆë¬¸ì¸ ê²½ìš° 2~7ë²ˆ ì ˆì°¨ì— ë”°ë¼ ë‹µë³€ì„ í•˜ì„¸ìš”.

2. [í•„ìˆ˜] ë‹µë³€ì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
   - ê´€ë ¨ ë²•ì  ê°œë… ì„¤ëª…
   - êµ¬ì²´ì ì¸ ë²•ì  ê·¼ê±°ì™€ í•´ì„
   - ì ìš© ê°€ëŠ¥í•œ ì¡°í•­ ì„¤ëª…

3. ì°¸ê³  ë²•ë ¹ì„ ëª…ì‹œí• ë•ŒëŠ” ë¬¸ì„œì˜ ì œëª©ì— ìˆëŠ” ë²•ë ¹ê³¼ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.         

4. [ì¤‘ìš”] ì œê³µëœ ë¬¸ì„œì— ê°™ì€ ë‚´ìš©ì´ ì—¬ëŸ¬ ë¬¸ì„œì— ì–¸ê¸‰ëœ ê²½ìš°, ë²•ì  íš¨ë ¥ì— ë”°ë¼ ë‹¤ìŒ ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ë‘ê³  ë‹µë³€í•˜ì„¸ìš”:
   - ë²•ë¥ 
   - ì‹œí–‰ë ¹
   - ì—…ë¬´í¸ëŒ ë“± ë¹„ë²•ë ¹ ìë£Œ
   * ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ê·œì •ì´ ë°œê²¬ë  ê²½ìš°, ë²•ì  íš¨ë ¥ì´ ë†’ì€ ë¬¸ì„œë¥¼ ìš°ì„ ìœ¼ë¡œ ì¸ìš©í•˜ê³ , í•„ìš” ì‹œ ë‹¤ë¥¸ ìë£ŒëŠ” ë³´ì¶© ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.             

5. [í˜•ì‹] ë²•ë¥ ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
   âš–ï¸ ê²°ë¡ : í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬
   ğŸ“Œ ì°¸ê³  ë²•ë ¹: ì¸ìš©ëœ ë²•ë ¹ ëª©ë¡

6. ë‹µë³€ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  
   ëª…ì‹œí•˜ì„¸ìš”.

7. [ì°¸ê³ ] ëª¨ë“  ì¡°í•­ ì¸ìš© ì‹œ "ã€Œë²•ë¥ ëª…ã€ ì œXì¡° ì œXí•­"ê³¼ ê°™ì´ ì •í™•í•œ ì¶œì²˜ë¥¼ 
   í‘œì‹œí•˜ì„¸ìš”.
        """
    ),
    ("user", "\n\n[ì œê³µëœ ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}")
])

# =========================
# 9. ë©”ì¸ í•¨ìˆ˜ ì •ì˜
# =========================

def main():
    # ë²¡í„°ìŠ¤í† ì–´ì™€ í•´ì‹œ íŒŒì¼ ì´ˆê¸°í™”
    vectorstore, hash_file_path = initialize_vectorstore(CONFIG["embedding_model"])

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ì „ì—­)
    global tokenizer
    try:
        tokenizer = tiktoken.encoding_for_model(CONFIG["model_name"])
    except KeyError:
        tokenizer = tiktoken.get_encoding("cl100k_base")

    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” - ëŒ€í™” ê¸°ë¡ ìœ ì§€ë¥¼ ìœ„í•´ í•„ìš”
    if 'memory' not in st.session_state:
        st.session_state['memory'] = ConversationBufferMemory(
            memory_key="chat_history",
            human_prefix="ì§ˆë¬¸",
            ai_prefix="ë‹µë³€",
            return_messages=True,
            output_key='answer'
        )

    # QA ì²´ì¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìƒì„±)
    if 'qa_chain' not in st.session_state:
        llm = ChatOpenAI(
            model_name=CONFIG["model_name"],
            temperature=0,
            streaming=True,
            openai_api_key=CONFIG["api_keys"]["openai"]
        )

        st.session_state['qa_chain'] = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
            memory=st.session_state['memory'],
            return_source_documents=True,
            chain_type="stuff",
            combine_docs_chain_kwargs={
                "prompt": chat_prompt,
                "document_variable_name": "context",
            },
            verbose=False,
            output_key='answer'                
        )
        logger.info("QA ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        callback_handler = st.session_state.get('callback_handler')

    # Streamlit ì•± íƒ€ì´í‹€
    st.title("ğŸ’¬ ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡")

    # ë©”ì‹œì§€ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ëª¨ë“  ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if question := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        try:
            # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = vectorstore.similarity_search(question, k=5)
            log_document_tokens(retrieved_docs)

            logger.info("\n=== ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
            logger.info(f"ì§ˆë¬¸: {question}")
            logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")

            # context ìƒì„±
            context = ""
            for i, doc in enumerate(retrieved_docs, 1):
                law_name = doc.metadata.get("law_name", "ì¶œì²˜ ì •ë³´ ì—†ìŒ")
                page = doc.metadata.get("page", "í˜ì´ì§€ ì •ë³´ ì—†ìŒ")
                content = doc.page_content

                context += f"\n\n ğŸ“œê´€ë ¨ë²•ë ¹ {i}] {law_name}, \nğŸ“„ í˜ì´ì§€ {page}: \nğŸ’¡ ë‚´ìš©:\n{content}\n"

                logger.info(f"\n[ë¬¸ì„œ {i}]")
                logger.info(f"ì¶œì²˜: {law_name}")
                logger.info(f"í˜ì´ì§€: {page}")
                logger.info(f"ë‚´ìš© ìš”ì•½: {content[:200]}...")

            logger.info("=== ê²€ìƒ‰ ê²°ê³¼ ì¢…ë£Œ ===\n")
            
            # ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                callback_handler = StreamlitCallbackHandler(message_placeholder)
                callback_manager = CallbackManager([callback_handler])
                
                # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì„ì‹œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_llm = ChatOpenAI(
                    model_name=CONFIG["model_name"],
                    temperature=0,
                    streaming=True,
                    openai_api_key=CONFIG["api_keys"]["openai"],
                    callback_manager=callback_manager
                )
                
                # ì„ì‹œ QA ì²´ì¸ ìƒì„±
                temp_qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=temp_llm,
                    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
                    memory=st.session_state['memory'],
                    return_source_documents=True,
                    chain_type="stuff",
                    combine_docs_chain_kwargs={
                        "prompt": chat_prompt,
                        "document_variable_name": "context",
                    },
                    verbose=False,
                    output_key='answer'                
                )
                
                response = temp_qa_chain({"question": question})
                
                # ì‘ë‹µì„ ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": response['answer']})

            logger.info("=== ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ ===\n")

        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {question}: {str(e)}")
            st.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()

if __name__ == "__main__":
    main()