import os
import sys
import openai
import logging
import warnings
import uuid
import traceback
import tempfile
import json
import hashlib
from pathlib import Path
from datetime import datetime
from threading import Lock
import time
import re
import streamlit as st
from dotenv import load_dotenv
import faiss 
import tiktoken
import fitz  # PyMuPDF

from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.docstore.document import Document
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from sentence_transformers import SentenceTransformer, util

# =========================
# 1. CONFIG ë”•ì…”ë„ˆë¦¬ ì •ì˜
# =========================

CONFIG = {
    "model_name": "gpt-4o-mini",  # LLM ëª¨ë¸ëª…
    "embedding_model": "text-embedding-3-large",  # ì„ë² ë”© ëª¨ë¸ëª…
    "embedding_size": 3072,  # ì„ë² ë”© ì°¨ì› í¬ê¸°
    "max_token_threshold": 4000,  # í† í° ìµœëŒ€ í•œë„
    "max_file_size": 20 * 1024 * 1024,  # 20MBë¡œ ì œí•œ
    "vectorstore_path": "faiss_vectorstore",  # ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    "hash_file_path": "faiss_vectorstore/document_hashes.json",  # ë¬¸ì„œ í•´ì‹œ íŒŒì¼ ê²½ë¡œ
    "chunk_size": 3000,  # ì²­í¬ í¬ê¸° 2000ì—ì„œ 3000ìœ¼ë¡œ ë³€ê²½
    "chunk_overlap": 150,  # ì²­í¬ ì¤‘ë³µ í¬ê¸°
    "api_keys": {
        "openai": None,  # OpenAI API í‚¤ë§Œ ë‚¨ê¹€
    }
}

# =========================
# 2. ë¡œê¹… ì„¤ì •
# =========================

# Custom Formatter for Logging with blank lines
class CustomFormatter(logging.Formatter):
    def format(self, record):
        # êµ¬ë¶„ì„  ìœ„ì™€ ì•„ë˜ì— ë¹ˆ ì¤„ ì¶”ê°€
        return f"\n--- {record.levelname} ---\n{super().format(record)}\n"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('chat_server.log', encoding='utf-8')
    ]
)

# í•¸ë“¤ëŸ¬ì— ì»¤ìŠ¤í…€ í¬ë§·í„° ì ìš©
for handler in logging.root.handlers:
    handler.setFormatter(CustomFormatter())

logger = logging.getLogger(__name__)

# DeprecationWarning ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

# =========================
# 3. API í‚¤ ë¡œë“œ
# =========================

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
CONFIG["api_keys"]["openai"] = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

# OpenAI API í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if not CONFIG["api_keys"]["openai"]:
    logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì´ë‚˜ Streamlit Cloud Secretsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = CONFIG["api_keys"]["openai"]

# =========================
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# =========================

def extract_law_name(file_name):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ë²•ë¥ ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    name = Path(file_name).stem
    name = re.sub(r'[_\-]', ' ', name)
    return name

def get_document_hash(doc_content: str, doc_name: str) -> str:
    """ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ë¦„ìœ¼ë¡œ í•´ì‹œê°’ ìƒì„±"""
    content = f"{doc_name}:{doc_content}".encode('utf-8')
    return hashlib.sha256(content).hexdigest()

def load_document_hashes(hash_file_path: Path) -> dict:
    """ì €ì¥ëœ ë¬¸ì„œ í•´ì‹œ ë¡œë“œ"""
    try:
        return json.loads(hash_file_path.read_text())
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def save_document_hashes(hash_file_path: Path, hashes: dict):
    """ë¬¸ì„œ í•´ì‹œ ì €ì¥"""
    try:
        hash_file_path.write_text(json.dumps(hashes, ensure_ascii=False, indent=2))
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

# =========================
# 5. ë¬¸ì„œ ë¶„í•  í´ë˜ìŠ¤ ì •ì˜
# =========================

class LawDocumentTextSplitter:
    def __init__(self, chunk_size=CONFIG["chunk_size"], chunk_overlap=CONFIG["chunk_overlap"]):
        self.chunker = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

    def split_documents(self, docs):
        law_sections = []
        for doc in docs:
            content = doc.page_content
            law_name = doc.metadata.get('law_name', 'ê´€ë ¨ ë²•ë¥ ')

            # RecursiveCharacterTextSplitterë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.chunker.split_text(content)

            for chunk in chunks:
                metadata = {
                    "law_name": law_name,
                    "page": doc.metadata.get('page', 'unknown')
                }
                law_sections.append(Document(page_content=chunk, metadata=metadata))

        return law_sections

# =========================
# 6. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í•¨ìˆ˜
# =========================

def initialize_vectorstore(embedding_model):
    VECTORSTORE_PATH = Path(CONFIG["vectorstore_path"])
    HASH_FILE_PATH = Path(CONFIG["hash_file_path"])

    VECTORSTORE_PATH.mkdir(parents=True, exist_ok=True)

    if not HASH_FILE_PATH.exists():
        HASH_FILE_PATH.write_text("{}")

    embeddings = OpenAIEmbeddings(model=embedding_model)

    if 'vectorstore' in st.session_state:
        logger.info("ì„¸ì…˜ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state['vectorstore'], HASH_FILE_PATH

    vectorstore = None
    vectorstore_files_exist = (VECTORSTORE_PATH / 'index.faiss').exists() and (VECTORSTORE_PATH / 'index.pkl').exists()

    if vectorstore_files_exist:
        try:
            vectorstore = LangChainFAISS.load_local(
                str(VECTORSTORE_PATH), 
                embeddings,
                allow_dangerous_deserialization=True
            )
            logger.info("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            logger.info(f"ë¡œë“œëœ ë²¡í„°ìŠ¤í† ì–´ì˜ ë¬¸ì„œ ìˆ˜: {vectorstore.index.ntotal}")
        except Exception as e:
            logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.info("ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            vectorstore = create_new_vectorstore(embeddings)
    else:
        logger.info("ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        vectorstore = create_new_vectorstore(embeddings)
        # ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ ìˆìœ¼ë¯€ë¡œ ì´ˆê¸° ë¡œë“œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        with st.spinner("ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘..."):
            initial_load(vectorstore)
            vectorstore.save_local(str(VECTORSTORE_PATH))

    st.session_state['vectorstore'] = vectorstore
    return vectorstore, HASH_FILE_PATH

def create_new_vectorstore(embeddings):
    embedding_size = CONFIG["embedding_size"]  # CONFIGì—ì„œ ì„ë² ë”© í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    index = faiss.IndexFlatL2(embedding_size)
    vectorstore = LangChainFAISS(
        embedding_function=embeddings,
        index=index,
        docstore=InMemoryDocstore({}),
        index_to_docstore_id={}
    )
    return vectorstore

# =========================
# 7. ë²¡í„°ìŠ¤í† ì–´ì™€ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” í•¨ìˆ˜
# =========================

def initialize_vectorstore_and_memory():
    vectorstore, hash_file_path = initialize_vectorstore(CONFIG["embedding_model"])

    if 'memory' not in st.session_state:
        st.session_state['memory'] = ConversationBufferMemory(
            memory_key="chat_history",
            human_prefix="ì§ˆë¬¸",
            ai_prefix="ë‹µë³€",
            return_messages=True,
            output_key='answer'
        )
    return vectorstore, hash_file_path

def reset_vectorstore():
    """ë²¡í„°ìŠ¤í† ì–´ì™€ ê´€ë ¨ íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ ì‚­ì œ
        vectorstore_path = Path(CONFIG["vectorstore_path"])
        if (vectorstore_path / "index.faiss").exists():
            (vectorstore_path / "index.faiss").unlink()
        if (vectorstore_path / "index.pkl").exists():
            (vectorstore_path / "index.pkl").unlink()
            
        # í•´ì‹œ íŒŒì¼ ì´ˆê¸°í™”
        hash_file_path = Path(CONFIG["hash_file_path"])
        if hash_file_path.exists():
            hash_file_path.write_text("{}")
            
        # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
        if 'vectorstore' in st.session_state:
            del st.session_state['vectorstore']
            
        logger.info("ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¦¬ì…‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# =========================
# 8. ì´ˆê¸° ë¡œë“œ ì‹œ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
# =========================

def load_all_pdfs_in_directory(directory_path):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ PyMuPDFë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logger.info("=== ë¬¸ì„œ ë¡œë“œ ì‹œì‘ ===")
    all_docs = []
    pdf_directory = Path(directory_path)
    pdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']

    for pdf_file in pdf_files:
        try:
            # PyMuPDFë¡œ PDF íŒŒì¼ ë¡œë“œ
            pdf_document = fitz.open(pdf_file)
            
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                text = page.get_text()
                
                if text.strip():  # ë¹ˆ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                    document = Document(
                        page_content=text,
                        metadata={
                            'law_name': extract_law_name(pdf_file.name),
                            'page': page_num + 1,
                            'source': str(pdf_file)
                        }
                    )
                    all_docs.append(document)
            
            pdf_document.close()
            logger.info(f"ë¬¸ì„œ '{pdf_file.name}' ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"âŒ {pdf_file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    logger.info("=== ë¬¸ì„œ ë¡œë“œ ì¢…ë£Œ ===")
    return all_docs

def initial_load(vectorstore):
    logger.info("=== ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸° ë¡œë“œ ì‹œì‘ ===")
    docs = []
    pdf_directory = Path('data')
    
    # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© í‘œì‹œ
    with st.sidebar:
        st.markdown("### ğŸ“Š ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”")
        progress_text = st.empty()
        progress_bar = st.progress(0)
        # ìƒíƒœ ë©”ì‹œì§€ë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
        status_messages = []
    
    pdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']
    total_files = len(pdf_files)
    
    for idx, pdf_file in enumerate(pdf_files):
        try:
            # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            with st.sidebar:
                progress_text.text(f"ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
                progress_bar.progress((idx + 1) / total_files)
            
            # PyMuPDFë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì„ ë¡œë“œ
            doc = fitz.open(pdf_file)
            content = "\n".join(page.get_text() for page in doc)
            if content:
                docs.append(Document(
                    page_content=content,
                    metadata={
                        'law_name': extract_law_name(pdf_file.name),
                        'page': 'unknown',
                        'source': str(pdf_file)
                    }
                ))
            # ìƒˆë¡œìš´ ìƒíƒœ ë©”ì‹œì§€ ì¶”ê°€
            with st.sidebar:
                status_messages.append(f"âœ… '{pdf_file.name}' ë¡œë“œ ì™„ë£Œ")
                # ëª¨ë“  ìƒíƒœ ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ í‘œì‹œ
                st.markdown("\n".join(status_messages))
            
            logger.info(f"ë¬¸ì„œ '{pdf_file.name}' ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€
            with st.sidebar:
                status_messages.append(f"âŒ {pdf_file.name} ì²˜ë¦¬ ì˜¤ë¥˜")
                st.markdown("\n".join(status_messages))
            logger.error(f"âŒ {pdf_file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    if docs:
        with st.sidebar:
            with st.spinner("ë¬¸ì„œ ë¶„í•  ë° ë²¡í„°í™” ì¤‘..."):
                text_splitter = LawDocumentTextSplitter(
                    chunk_size=CONFIG["chunk_size"], 
                    chunk_overlap=CONFIG["chunk_overlap"]
                )
                splits = text_splitter.split_documents(docs)
                vectorstore.add_documents(splits)
                VECTORSTORE_PATH = Path(CONFIG["vectorstore_path"])
                vectorstore.save_local(str(VECTORSTORE_PATH))
                
                # ìµœì¢… ê²°ê³¼ í‘œì‹œ
                progress_text.empty()
                progress_bar.empty()
                st.success(f"""
                âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ
                - ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ
                - ì´ ì²­í¬ ìˆ˜: {len(splits)}ê°œ
                """)

    logger.info("=== ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸° ë¡œë“œ ì¢…ë£Œ ===")

# =========================
# 9. íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ í•¨ìˆ˜
# =========================

def process_uploaded_files(uploaded_files, vectorstore, hash_file_path):
    logger.info("=== ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===")
    document_hashes = load_document_hashes(hash_file_path)
    skipped_files, processed_files = [], []

    # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© í‘œì‹œ
    with st.sidebar:
        upload_status = st.empty()  # ì—…ë¡œë“œ ìƒíƒœ ì„¹ì…˜ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ
        with upload_status.container():  # ëª¨ë“  ì—…ë¡œë“œ ê´€ë ¨ UIë¥¼ í•˜ë‚˜ì˜ ì»¨í…Œì´ë„ˆë¡œ ë¬¶ìŒ
            st.markdown("### ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì§„í–‰ìƒí™©")
            progress_text = st.empty()
            progress_bar = st.progress(0)
            status_messages = []
            status_container = st.empty()

    total_files = len(uploaded_files)
    
    for idx, uploaded_file in enumerate(uploaded_files):
        try:
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            with st.sidebar:
                progress_text.text(f"ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
                progress_bar.progress((idx + 1) / total_files)

            if uploaded_file.size > CONFIG["max_file_size"]:
                status_messages.append(f"âŒ {uploaded_file.name}: íŒŒì¼ í¬ê¸° ì´ˆê³¼")
                status_container.markdown("\n".join(status_messages))
                continue

            if uploaded_file.type != "application/pdf":
                status_messages.append(f"âŒ {uploaded_file.name}: PDF íŒŒì¼ì´ ì•„ë‹˜")
                status_container.markdown("\n".join(status_messages))
                continue

            # íŒŒì¼ ì²˜ë¦¬ ë¡œì§...
            temp_path = Path(tempfile.gettempdir()) / f"{uuid.uuid4().hex}.pdf"
            
            with open(temp_path, 'wb') as f:
                f.write(uploaded_file.getbuffer())
            
            # ë¬¸ì„œ í•´ì‹œ ê³„ì‚°
            pdf_document = fitz.open(temp_path)
            full_text = ""
            for page in pdf_document:
                full_text += page.get_text()
            
            doc_hash = get_document_hash(full_text, uploaded_file.name)

            if doc_hash in document_hashes:
                status_messages.append(f"â„¹ï¸ {uploaded_file.name}: ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼")
                status_container.markdown("\n".join(status_messages))
                skipped_files.append(uploaded_file.name)
                continue

            # ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
            documents = []
            for page_num in range(len(pdf_document)):
                text = pdf_document[page_num].get_text()
                if text.strip():
                    documents.append(Document(
                        page_content=text,
                        metadata={
                            "source": uploaded_file.name,
                            "page": page_num + 1,
                            "doc_hash": doc_hash,
                            "law_name": extract_law_name(uploaded_file.name),
                            "processed_date": datetime.now().isoformat()
                        }
                    ))

            # ë¬¸ì„œ ë¶„í•  ë° ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
            text_splitter = LawDocumentTextSplitter(
                chunk_size=CONFIG["chunk_size"],
                chunk_overlap=CONFIG["chunk_overlap"]
            )
            split_docs = text_splitter.split_documents(documents)
            vectorstore.add_documents(split_docs)

            # ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
            document_hashes[doc_hash] = {
                "filename": uploaded_file.name,
                "pages": len(documents),
                "total_chunks": len(split_docs),
                "processed_date": datetime.now().isoformat()
            }
            processed_files.append(uploaded_file.name)
            
            # ì„±ê³µ ë©”ì‹œì§€ ì¶”ê°€
            status_messages.append(f"âœ… {uploaded_file.name}: ì²˜ë¦¬ ì™„ë£Œ ({len(split_docs)} ì²­í¬)")
            status_container.markdown("\n".join(status_messages))

        except Exception as e:
            status_messages.append(f"âŒ {uploaded_file.name}: ì²˜ë¦¬ ì‹¤íŒ¨")
            status_container.markdown("\n".join(status_messages))
            logger.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if 'pdf_document' in locals():
                pdf_document.close()
            if temp_path and temp_path.exists():
                temp_path.unlink()

    # ì²˜ë¦¬ ì™„ë£Œ í›„
    if processed_files:
        vectorstore.save_local(CONFIG["vectorstore_path"])
        save_document_hashes(hash_file_path, document_hashes)
        with upload_status.container():
            st.success(f"""
            âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ
            - ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼: {len(processed_files)}ê°œ
            - ê±´ë„ˆë›´ íŒŒì¼: {len(skipped_files)}ê°œ
            """)
        
        # 3ì´ˆ í›„ ì—…ë¡œë“œ ìƒíƒœ ë©”ì‹œì§€ ì œê±°
        time.sleep(3)
        upload_status.empty()

    logger.info("=== ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ì¢…ë£Œ ===")
    return processed_files, skipped_files

# =========================
# 10. í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
# =========================

def log_document_tokens(docs):
    """
    ê° ë¬¸ì„œì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    total_tokens = 0
    for i, doc in enumerate(docs, 1):
        tokens = len(tokenizer.encode(doc.page_content))
        total_tokens += tokens
        logger.debug(f"[ë¬¸ì„œ {i}] í† í° ìˆ˜: {tokens}")

    logger.debug(f"ì´ ë¬¸ì„œ í† í° ìˆ˜: {total_tokens}")
    return total_tokens

# =========================
# 11. ChatPromptTemplate ì •ì˜
# =========================

chat_prompt = ChatPromptTemplate.from_messages([
     (
      "system",
         """ë‹¹ì‹ ì€ í•œêµ­ì˜ ìµœê³  ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

        1. [ì‚¬ì „ê²€í† ] ì§ˆë¬¸ë‚´ìš©ì´ ë²•ë¥ ê³¼ ê´€ë ¨ì´ ì—†ìœ¼ë©´ ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”. 
        
        2. [í•„ìˆ˜] ë‹µë³€ì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
        - ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
        - ê´€ë ¨ ë²•ì  ê°œë… ì„¤ëª…
        - êµ¬ì²´ì ì¸ ë²•ì  ê·¼ê±°ì™€ í•´ì„
        - ì ìš© ê°€ëŠ¥í•œ ì¡°í•­ê³¼ **ë‹¨ì„œì¡°í•­**ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª… (íŠ¹íˆ ì œí•œì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ì‚¬í•­ì´ ìˆëŠ” ê²½ìš° ì´ë¥¼ ê°•ì¡°)

        3. ì°¸ê³  ë²•ë ¹ì„ ëª…ì‹œí• ë•ŒëŠ” ë¬¸ì„œì˜ ì œëª©ì— ìˆëŠ” ë²•ë ¹ê³¼ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.         

        4. [ì¤‘ìš”] ê²€ìƒ‰ëœ ë¬¸ì„œì— ê°™ì€ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°, ë²•ì  íš¨ë ¥ì— ë”°ë¼ ë‹¤ìŒ ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ë‘ê³  ë‹µë³€í•˜ì„¸ìš”:
        - ë²•ë¥ 
        - ì‹œí–‰ë ¹
        - ì—…ë¬´í¸ëŒ ë“± ë¹„ë²•ë ¹ ìë£Œ
        * ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ê·œì •ì´ ë°œê²¬ë  ê²½ìš°, ë²•ì  íš¨ë ¥ì´ ë†’ì€ ë¬¸ì„œë¥¼ ìš°ì„ ìœ¼ë¡œ ì¸ìš©í•˜ê³ , í•„ìš” ì‹œ ë‹¤ë¥¸ ìë£ŒëŠ” ë³´ì¶© ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.             

        5. [í˜•ì‹] ë²•ë¥ ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
        âš–ï¸ ê²°ë¡ : í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬
        ğŸ“Œ ì°¸ê³  ë²•ë ¹: ì¸ìš©ëœ ë²•ë ¹ ëª©ë¡

        6. ë‹µë³€ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

        7. [ì°¸ê³ ] ëª¨ë“  ì¡°í•­ ì¸ìš© ì‹œ "ã€Œë²•ë¥ ëª…ã€ ì œXì¡° ì œXí•­"ê³¼ ê°™ì´ ì •í™•í•œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
                """
    ),
    ("user", "{chat_history}\n\n[ì œê³µëœ ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}")
])

# =========================
# 12. ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ì •ì˜
# =========================

class StreamlitCallbackHandler(BaseCallbackHandler): 
    """Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ í‘œì‹œí•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬."""

    def __init__(self, message_placeholder):
        self.message_placeholder = message_placeholder
        self.answer_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        skip_patterns = ["Human:", "Assistant:", "ì§ˆë¬¸: ", "ë‹µë³€: "]
        if any(pattern in token for pattern in skip_patterns):
            return
            
        self.answer_text += token
        self.message_placeholder.markdown(self.answer_text + "â–Œ")

    def on_llm_end(self, response, **kwargs):
        """LLM ì‘ë‹µ ì™„ë£Œë˜ë©´ í˜¸ì¶œë©ë‹ˆë‹¤."""
        final_answer = self.answer_text.strip()
        self.message_placeholder.markdown(final_answer)

# =========================
# 12-1. qa_chain ì´ˆê¸°í™” í•¨ìˆ˜ ì¶”ê°€
# =========================

def initialize_qa_chain(vectorstore):
    """QA Chainì„ ì´ˆê¸°í™”í•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì €ì¥"""
    if 'qa_chain' not in st.session_state or st.session_state['qa_chain'] is None:
        # ì´ˆê¸°í™” ì‹œ ë‹¨ì¼ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        message_placeholder = st.empty()
        callback_handler = StreamlitCallbackHandler(message_placeholder)
        callback_manager = CallbackManager([callback_handler])

        llm = ChatOpenAI(
            model_name=CONFIG["model_name"],
            temperature=0,
            streaming=True,
            openai_api_key=CONFIG["api_keys"]["openai"],
            callback_manager=callback_manager
        )

        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
            memory=st.session_state['memory'],
            return_source_documents=True,
            chain_type="stuff",
            combine_docs_chain_kwargs={
                "prompt": chat_prompt,
                "document_variable_name": "context",
            },
            verbose=False,
            output_key='answer'
        )
        st.session_state['qa_chain'] = qa_chain
        st.session_state['callback_handler'] = callback_handler  # í˜„ì¬ í•¸ë“¤ëŸ¬ ì €ì¥

    return st.session_state['qa_chain'], st.session_state['callback_handler']


def is_relevant_question(question, vectorstore, threshold=0.5):
    """ì§ˆë¬¸ì´ ë²¡í„°ìŠ¤í† ì–´ì˜ ë¬¸ì„œì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    # ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
    question_embedding = model.encode(question, convert_to_tensor=True)

    # ë²¡í„°ìŠ¤í† ì–´ì˜ ë¬¸ì„œ ì„ë² ë”©ê³¼ ìœ ì‚¬ì„± ê³„ì‚°
    cosine_similarities = util.pytorch_cos_sim(question_embedding, vectorstore.embeddings)

    # ìœ ì‚¬ì„± ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê´€ë ¨ì„± íŒë‹¨
    return cosine_similarities.max().item() > threshold


# =========================
# 13. ë©”ì¸ í•¨ìˆ˜ ì •ì˜
# =========================

def main():
    # ====================
    # 1. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    # ====================
    if 'generated' not in st.session_state:
        st.session_state['generated'] = []
    if 'past' not in st.session_state:
        st.session_state['past'] = []
    if 'qa_chain' not in st.session_state:
        st.session_state['qa_chain'] = None
    if 'callback_handler' not in st.session_state:
        st.session_state['callback_handler'] = None

    # ë²¡í„°ìŠ¤í† ì–´ì™€ í•´ì‹œ íŒŒì¼, ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    vectorstore, hash_file_path = initialize_vectorstore_and_memory()

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ì „ì—­)
    global tokenizer
    tokenizer = tiktoken.encoding_for_model(CONFIG["model_name"])

    with st.sidebar:
        st.header("ğŸ“„ ë¬¸ì„œ ê´€ë¦¬")
        
        # ë¦¬ì…‹ ë²„íŠ¼
        if st.button("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”", help="ì²­í¬ ì‚¬ì´ì¦ˆ ë³€ê²½ ë“±ì„ ìœ„í•´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"):
            if reset_vectorstore():
                st.success("ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
                st.info("ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                st.error("ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        # êµ¬ë¶„ì„  ì¶”ê°€
        st.markdown("---")
        
        # íŒŒì¼ ì—…ë¡œë”
        st.subheader("ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=["pdf"],
            accept_multiple_files=True,
            key="uploader",
            help="ìµœëŒ€ ì—…ë¡œë“œ í¬ê¸°: 20MB",
        )

        if uploaded_files:
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                processed_files, skipped_files = process_uploaded_files(
                    uploaded_files, vectorstore, hash_file_path
                )

        # êµ¬ë¶„ì„  ì¶”ê°€
        st.markdown("---")
        
        # ë¬¸ì„œ í†µê³„
        st.subheader("ğŸ“Š ë¬¸ì„œ í†µê³„")
        document_hashes = load_document_hashes(hash_file_path)
        if document_hashes:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ë¬¸ì„œ ìˆ˜", f"{len(document_hashes)}ê°œ")
            with col2:
                total_pages = sum(doc_info.get('pages', 0) for doc_info in document_hashes.values())
                st.metric("ì´ í˜ì´ì§€ ìˆ˜", f"{total_pages}ê°œ")
            with col3:
                total_chunks = sum(doc_info.get('total_chunks', 0) for doc_info in document_hashes.values())
                st.metric("ì´ ì²­í¬ ìˆ˜", f"{total_chunks}ê°œ")

    st.markdown("<h3 style='text-align: center;'>ğŸ’¬ ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡</h3>", unsafe_allow_html=True)

    # ê³¼ê±° ëŒ€í™” í‘œì‹œ
    for i in range(len(st.session_state['generated'])):
        st.chat_message("user").markdown(st.session_state['past'][i])
        st.chat_message("assistant").markdown(st.session_state['generated'][i])

    # QA Chain ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë¨) ë° ì½œë°± í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
    qa_chain, callback_handler = initialize_qa_chain(vectorstore)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if question := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("user").markdown(question)
        st.session_state['past'].append(question)

        try:
            # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = vectorstore.similarity_search(question, k=5)
            log_document_tokens(retrieved_docs)

            logger.info("\n=== ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
            logger.info(f"ì§ˆë¬¸: {question}")
            logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")

            # context ìƒì„± ì½”ë“œ
            context = ""
            for i, doc in enumerate(retrieved_docs, 1):
                sources = doc.metadata.get("law_name", "ì¶œì²˜ ì •ë³´ ì—†ìŒ")
                page = doc.metadata.get("page", "í˜ì´ì§€ ì •ë³´ ì—†ìŒ")
                context += f"\n\n ğŸ“œê´€ë ¨ë²•ë ¹ {i}] {sources}, \nğŸ“„ í˜ì´ì§€ {page}: \nğŸ’¡ ë‚´ìš©:\n{doc.page_content}\n"
                        
                logger.info(f"\n[ë¬¸ì„œ {i}]")
                logger.info(f"[ì…ë ¥ ì „ì²´ ì •ë³´ {context}]")
                logger.info(f"ì¶œì²˜: {doc.metadata.get('law_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                logger.info(f"í˜ì´ì§€: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                logger.info(f"ë‚´ìš© ìš”ì•½: {doc.page_content[:200]}...")

            logger.info("=== ê²€ìƒ‰ ê²°ê³¼ ì¢…ë£Œ ===\n")
            
            # ì½œë°± í•¸ë“¤ëŸ¬ì˜ message_placeholder ì—…ë°ì´íŠ¸
            callback_handler.message_placeholder = st.empty()
            callback_handler.answer_text = ""  # ì´ì „ ì‘ë‹µ ì´ˆê¸°í™”

            # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
            response = qa_chain({"question": question})
            source_docs = response.get('source_documents', [])
            
            # ìƒì„±ëœ ë‹µë³€ì„ ì„¸ì…˜ì— ì €ì¥
            st.session_state['generated'].append(callback_handler.answer_text)
            logger.info("=== ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ ===\n")

        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {question}: {str(e)}")
            st.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            st.session_state['generated'].append("ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            traceback.print_exc()

# =========================
# 14. ì‹¤í–‰ ì‹œì‘
# =========================

if __name__ == "__main__":
    main()
