# app.py

import os
import sys
import openai
import logging
import warnings
import json
import re
import traceback
from pathlib import Path
from datetime import datetime
import faiss
import streamlit as st
from dotenv import load_dotenv
import tiktoken

from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
from langchain.docstore.document import Document


# =========================
# 1. CONFIG ë”•ì…”ë„ˆë¦¬ ì •ì˜
# =========================

CONFIG = {
    "model_name": "gpt-4o-mini",  # LLM ëª¨ë¸ëª… (ì˜¤íƒ€ ìˆ˜ì •: "gpt-4o" â†’ "gpt-4")
    "embedding_model": "text-embedding-3-large",  # ì„ë² ë”© ëª¨ë¸ëª… (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •)
    "vectorstore_path": "faiss_vectorstore",  # ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    "hash_file_path": "faiss_vectorstore/document_hashes.json",  # ë¬¸ì„œ í•´ì‹œ íŒŒì¼ ê²½ë¡œ
    "api_keys": {
        "openai": None,  # OpenAI API í‚¤
    }
}

# =========================
# 2. ë¡œê¹… ì„¤ì •
# =========================

class CustomFormatter(logging.Formatter):
    def format(self, record):
        return f"\n--- {record.levelname} ---\n{super().format(record)}\n"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('chat_server.log', encoding='utf-8')
    ]
)

for handler in logging.root.handlers:
    handler.setFormatter(CustomFormatter())

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=DeprecationWarning)

# =========================
# 3. API í‚¤ ë¡œë“œ
# =========================

load_dotenv()

CONFIG["api_keys"]["openai"] = os.getenv("OPENAI_API_KEY")

if not CONFIG["api_keys"]["openai"]:
    logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

openai.api_key = CONFIG["api_keys"]["openai"]

# =========================
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# =========================

def extract_law_name(file_name):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ë²•ë¥ ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    name = Path(file_name).stem
    name = re.sub(r'[_\-]', ' ', name)
    return name

def load_document_hashes(hash_file_path: Path) -> dict:
    """ì €ì¥ëœ ë¬¸ì„œ í•´ì‹œ ë¡œë“œ"""
    try:
        return json.loads(hash_file_path.read_text())
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

# =========================
# 5. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í•¨ìˆ˜
# =========================

def initialize_vectorstore(embedding_model):
    VECTORSTORE_PATH = Path(CONFIG["vectorstore_path"])
    HASH_FILE_PATH = Path(CONFIG["hash_file_path"])

    embeddings = OpenAIEmbeddings(model=embedding_model, openai_api_key=CONFIG["api_keys"]["openai"])

    if 'vectorstore' in st.session_state:
        logger.info("ì„¸ì…˜ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state['vectorstore'], HASH_FILE_PATH

    try:
        vectorstore = LangChainFAISS.load_local(
            str(VECTORSTORE_PATH), 
            embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    st.session_state['vectorstore'] = vectorstore
    return vectorstore, HASH_FILE_PATH

# =========================
# 6. ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ì •ì˜
# =========================

class StreamlitCallbackHandler(BaseCallbackHandler):
    """Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ í‘œì‹œí•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬."""

    def __init__(self, message_placeholder):
        self.message_placeholder = message_placeholder
        self.answer_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        skip_patterns = ["Human:", "Assistant:", "ì§ˆë¬¸: ", "ë‹µë³€: "]
        if any(pattern in token for pattern in skip_patterns):
            return

        self.answer_text += token
        self.message_placeholder.markdown(self.answer_text + "â–Œ")

    def on_llm_end(self, response, **kwargs):
        """LLM ì‘ë‹µ ì™„ë£Œë˜ë©´ í˜¸ì¶œë©ë‹ˆë‹¤."""
        final_answer = self.answer_text.strip()
        self.message_placeholder.markdown(final_answer)
        self.answer_text = ""

# =========================
# 7. í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
# =========================

def log_document_tokens(docs):
    """
    ê° ë¬¸ì„œì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    total_tokens = 0
    for i, doc in enumerate(docs, 1):
        tokens = len(tokenizer.encode(doc.page_content))
        total_tokens += tokens
        logger.debug(f"[ë¬¸ì„œ {i}] í† í° ìˆ˜: {tokens}")

    logger.debug(f"ì´ ë¬¸ì„œ í† í° ìˆ˜: {total_tokens}")
    return total_tokens

# =========================
# 8. ChatPromptTemplate ì •ì˜
# =========================

chat_prompt = ChatPromptTemplate.from_messages([
    (
        "system",

        """ë‹¹ì‹ ì€ í•œêµ­ì˜ ìµœê³  ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. êµ­ë¦½ìˆ˜ì‚°ê³¼í•™ì›ì˜ ê·¼ë¬´ìë“¤ì„ ìœ„í•´, **ì²­ë ´**(ë°˜ë¶€íŒ¨, ìœ¤ë¦¬ ë“±) ê´€ë ¨ ë²•ë¥  ìƒë‹´ë§Œì„ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

        1. **[ì¤‘ìš”]** ì§ˆë¬¸ì´ ì²­ë ´, ë³µë¬´ ë“±ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸**ì¸ ê²½ìš°ì—ë§Œ ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.

        2. **[í•„ìˆ˜]** ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”:
        - ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
        - ê´€ë ¨ ë²•ì  ê°œë… ì„¤ëª…
        - êµ¬ì²´ì ì¸ ë²•ì  ê·¼ê±°ì™€ í•´ì„
        - ì ìš© ê°€ëŠ¥í•œ ì¡°í•­ ì„¤ëª…
        - **ë‹¨ì„œì¡°í•­**, **ì˜ˆì™¸ì‚¬í•­** ë“±ì€ ëª…í™•íˆ ëª…ì‹œ

        3. **[ì°¸ê³  ë²•ë ¹ ëª…ì‹œ]** ì°¸ê³  ë²•ë ¹ì„ ëª…ì‹œí•  ë•ŒëŠ” **ë²•ë ¹ì˜ ì œëª©**ê³¼ **ì¡°í•­**ì„ ì •í™•íˆ ëª…ì‹œí•˜ì„¸ìš”.
        - ì˜ˆì‹œ: "ì´ëŠ” ã€Œê³µì§ììœ¤ë¦¬ë²•ã€ ì œ5ì¡° ì œ2í•­ì— ê·¼ê±°í•©ë‹ˆë‹¤."

        4. **[ë²•ì  íš¨ë ¥ ìš°ì„ ìˆœìœ„]** ì œê³µëœ ë¬¸ì„œì— ê°™ì€ ë‚´ìš©ì´ ì—¬ëŸ¬ ë²ˆ ì–¸ê¸‰ëœ ê²½ìš°, ë²•ì  íš¨ë ¥ì— ë”°ë¼ ë‹¤ìŒ ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ë‘ê³  ë‹µë³€í•˜ì„¸ìš”:
        - **ë²•ë¥ **
        - **ì‹œí–‰ë ¹**
        - **ì—…ë¬´í¸ëŒ ë“± ë¹„ë²•ë ¹ ìë£Œ**
        - ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ê·œì •ì´ ë°œê²¬ë  ê²½ìš°, ë²•ì  íš¨ë ¥ì´ ë†’ì€ ë¬¸ì„œë¥¼ ìš°ì„ ìœ¼ë¡œ ì¸ìš©í•˜ê³ , í•„ìš” ì‹œ ë‹¤ë¥¸ ìë£ŒëŠ” ë³´ì¶© ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.

        5. **[í˜•ì‹]** ì²­ë ´ ê´€ë ¨ ë²•ë¥ ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•  ê²½ìš°ì—ë§Œ, ë‹µë³€ ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
        - âš–ï¸ **ê²°ë¡ **: í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬
        - ğŸ“Œ **ì°¸ê³  ë²•ë ¹**: ì¸ìš©ëœ ë²•ë ¹ ëª©ë¡

        6. **[ì°¸ê³  ë²•ë ¹ í˜•ì‹]** ê²°ë¡ ê³¼ 'ì°¸ê³  ë²•ë ¹' ëª©ë¡ ì‚¬ì´ì— ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆì„ í•˜ì„¸ìš”.

        7. **[ì§ˆë¬¸ ë¶„ë¥˜]** ì§ˆë¬¸ì´ ì²­ë ´ê³¼ ê´€ë ¨ëœ ë²•ë¥ ì ì¸ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°, ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”:
        - **ì²­ë ´ê³¼ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¸ ê²½ìš°**: "ì£„ì†¡í•˜ì§€ë§Œ, ë³¸ ì±—ë´‡ì€ ì²­ë ´ ê´€ë ¨ ë²•ë¥  ìƒë‹´ì—ë§Œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤."
        - **ì¸ì‚¬ë§ ë“± small talkì€ **ë‹µë³€í•˜ì§€ë§Œ, ì§§ê²Œ ì¸ì‚¬ë§ë§Œ í•˜ê³  "ë³¸ ì±—ë´‡ì€ ì²­ë ´ ê´€ë ¨ ë²•ë¥  ìƒë‹´ì—ë§Œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤."ë¼ê³  í•˜ì„¸ìš”.
        - **ì²­ë ´ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì¸ ê²½ìš°**: "ì£„ì†¡í•˜ì§€ë§Œ, ë³¸ ì±—ë´‡ì€ ì²­ë ´ ê´€ë ¨ ë²•ë¥  ìƒë‹´ì„ ìœ„í•œ ì±—ë´‡ì…ë‹ˆë‹¤."

        8. **[ì¶œì²˜ í‘œê¸°]** ëª¨ë“  ì¡°í•­ ì¸ìš© ì‹œ "ã€Œë²•ë¥ ëª…ã€ ì œXì¡° ì œXí•­"ê³¼ ê°™ì´ ì •í™•í•œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
        
        """
    ),
    ("user", "\n\n[ì œê³µëœ ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}")
])

# =========================
# 9. ë©”ì¸ í•¨ìˆ˜ ì •ì˜
# =========================

# í˜ì´ì§€ ì„¤ì •ì„ ë©”ì¸ í•¨ìˆ˜ ë°–ìœ¼ë¡œ ì´ë™
import streamlit as st
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# í…Œë§ˆ ì„¤ì •
current_hour = datetime.now().hour
is_dark_mode = current_hour < 6 or current_hour >= 18

if is_dark_mode:
    bg_color = "#1a1a1a"
    text_color = "#ffffff"
    header_bg = "#2d2d2d"
    chat_bg = "#1a1a1a"
    user_msg_bg = "#4a4a4a"
    assistant_msg_bg = "#2d2d2d"
    input_bg = "#2d2d2d"
else:
    bg_color = "#f5f6f7"
    text_color = "#1a1a1a"
    header_bg = "#ffffff"
    chat_bg = "#f5f6f7"
    user_msg_bg = "#007AFF"
    assistant_msg_bg = "#ffffff"
    input_bg = "#ffffff"

# CSS ìŠ¤íƒ€ì¼
st.markdown(f"""
    <style>
    /* ê¸°ë³¸ Streamlit ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {{visibility: hidden;}}
    header {{visibility: hidden;}}
    footer {{visibility: hidden;}}
    [data-testid="stToolbar"] {{display: none !important;}}
    
    /* ì „ì²´ ë°°ê²½ìƒ‰ ì„¤ì • */
    .stApp {{
        background-color: {bg_color};
        max-width: 100vw;
        overflow-x: hidden;
    }}
    
    /* ìƒë‹¨ í—¤ë” ìŠ¤íƒ€ì¼ */
    .fixed-header {{
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        height: 50px;
        background: {header_bg};
        display: flex;
        align-items: center;
        justify-content: center;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        z-index: 1000;
    }}
    
    .header-title {{
        font-size: 1rem;
        color: {text_color};
        font-weight: 500;
        display: flex;
        align-items: center;
        gap: 6px;
    }}
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .stChatFloatingInputContainer {{
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        padding: 8px;
        background: {input_bg};
        border-top: 1px solid rgba(0,0,0,0.1);
        z-index: 999;
    }}
    
    /* ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .stChatMessageContent {{
        padding: 10px 12px;
        border-radius: 18px;
        max-width: 85%;
        margin: 4px 8px;
        font-size: 0.95rem;
        word-break: break-word;
    }}
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    [data-testid="StChatMessage"][data-testid="user"] {{
        justify-content: flex-end;
        padding-left: 10%;
    }}
    
    [data-testid="StChatMessage"][data-testid="user"] .stChatMessageContent {{
        background: {user_msg_bg};
        color: #ffffff;
        margin-left: auto;
        border-bottom-right-radius: 4px;
    }}
    
    /* ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    [data-testid="StChatMessage"]:not([data-testid="user"]) {{
        padding-right: 10%;
    }}

    [data-testid="StChatMessage"]:not([data-testid="user"]) .stChatMessageContent {{
        background: {assistant_msg_bg};
        color: {text_color};
        border-bottom-left-radius: 4px;
        margin-right: auto;
    }}
    
    /* ì–´ì‹œìŠ¤í„´íŠ¸ ì•„ì´ì½˜ ìŠ¤íƒ€ì¼ */
    [data-testid="StChatMessage"]:not([data-testid="user"]) .stChatMessageAvatar {{
        background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="%23666666"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z"/></svg>');
        background-size: 65%;
        background-position: center;
        background-repeat: no-repeat;
        background-color: transparent;
        width: 28px;
        height: 28px;
        margin-right: 6px;
    }}
    
    /* ì‚¬ìš©ì ì•„ì´ì½˜ ìˆ¨ê¸°ê¸° */
    [data-testid="StChatMessage"][data-testid="user"] .stChatMessageAvatar {{
        display: none;
    }}
    
    /* ì±„íŒ… ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stChatInputContainer {{
        background: {input_bg};
        border-radius: 20px;
        margin: 0 8px;
        padding: 6px 12px;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }}
    
    textarea {{
        border: none !important;
        background: transparent !important;
        padding: 8px !important;
        font-size: 0.95rem !important;
        line-height: 1.4 !important;
        max-height: 100px !important;
    }}
    
    /* ìŠ¤í¬ë¡¤ë°” ìŠ¤íƒ€ì¼ */
    ::-webkit-scrollbar {{
        width: 4px;
    }}
    
    ::-webkit-scrollbar-track {{
        background: transparent;
    }}
    
    ::-webkit-scrollbar-thumb {{
        background: rgba(0,0,0,0.2);
        border-radius: 2px;
    }}
    
    /* ì±„íŒ… ì˜ì—­ ì—¬ë°± ì¡°ì • */
    .main .block-container {{
        padding-top: 60px;
        padding-bottom: 80px;
        padding-left: 0;
        padding-right: 0;
    }}

    /* ëª¨ë°”ì¼ ìµœì í™” */
    @media (max-width: 768px) {{
        .stChatMessageContent {{
            max-width: 90%;
            font-size: 0.9rem;
            padding: 8px 12px;
        }}
        
        .header-title {{
            font-size: 0.95rem;
        }}
        
        .stChatInputContainer {{
            margin: 0 4px;
        }}
        
        textarea {{
            font-size: 0.9rem !important;
        }}
    }}
    </style>
    
    <div class="fixed-header">
        <div class="header-title">
            <span>âš–ï¸</span>
            <span>ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡</span>
        </div>
    </div>
""", unsafe_allow_html=True)


def main():
    
    
    # ìŠ¤íƒ€ì¼ ì„¤ì • ì ìš©
    
    
    # ë²¡í„°ìŠ¤í† ì–´ì™€ í•´ì‹œ íŒŒì¼ ì´ˆê¸°í™”
    vectorstore, hash_file_path = initialize_vectorstore(CONFIG["embedding_model"])

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ì „ì—­)
    global tokenizer
    try:
        tokenizer = tiktoken.encoding_for_model(CONFIG["model_name"])
    except KeyError:
        tokenizer = tiktoken.get_encoding("cl100k_base")

    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” - ëŒ€í™” ê¸°ë¡ ìœ ì§€ë¥¼ ìœ„í•´ í•„ìš”
    if 'memory' not in st.session_state:
        st.session_state['memory'] = ConversationBufferMemory(
            memory_key="chat_history",
            human_prefix="ì§ˆë¬¸",
            ai_prefix="ë‹µë³€",
            return_messages=True,
            output_key='answer'
        )

    # QA ì²´ì¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìƒì„±)
    if 'qa_chain' not in st.session_state:
        llm = ChatOpenAI(
            model_name=CONFIG["model_name"],
            temperature=0,
            streaming=True,
            openai_api_key=CONFIG["api_keys"]["openai"]
        )

        st.session_state['qa_chain'] = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
            memory=st.session_state['memory'],
            return_source_documents=True,
            chain_type="stuff",
            combine_docs_chain_kwargs={
                "prompt": chat_prompt,
                "document_variable_name": "context",
            },
            verbose=False,
            output_key='answer'                
        )
        logger.info("QA ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        callback_handler = st.session_state.get('callback_handler')

    # ë©”ì¸ ì»¨í…Œì´ë„ˆ ì‹œì‘
    st.markdown('<div class="main-container">', unsafe_allow_html=True)
    
    # ì±„íŒ… ì˜ì—­ ì‹œì‘
    st.markdown('<div class="chat-area">', unsafe_allow_html=True)
    
    # ë©”ì‹œì§€ í‘œì‹œ
    if "messages" not in st.session_state:
        st.session_state.messages = []
        welcome_msg = {
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”. ğŸ˜Š"
        }
        st.session_state.messages.append(welcome_msg)

    # ëª¨ë“  ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ì…ë ¥ì°½
    st.markdown('<div class="input-container">', unsafe_allow_html=True)
    if question := st.chat_input("ğŸ’­ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        try:
            # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = vectorstore.similarity_search(question, k=5)
            log_document_tokens(retrieved_docs)

            logger.info("\n=== ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
            logger.info(f"ì§ˆë¬¸: {question}")
            logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")

            # context ìƒì„±
            context = ""
            for i, doc in enumerate(retrieved_docs, 1):
                law_name = doc.metadata.get("law_name", "ì¶œì²˜ ì •ë³´ ì—†ìŒ")
                page = doc.metadata.get("page", "í˜ì´ì§€ ì •ë³´ ì—†ìŒ")
                content = doc.page_content

                context += f"\n\n ê´€ë ¨ë²•ë ¹ {i}] {law_name}, \nğŸ“„ í˜ì´ì§€ {page}: \nğŸ’¡ ë‚´ìš©:\n{content}\n"

                logger.info(f"\n[ë¬¸ì„œ {i}]")
                logger.info(f"ì¶œì²˜: {law_name}")
                logger.info(f"í˜ì´ì§€: {page}")
                logger.info(f"ë‚´ìš© ìš”ì•½: {content[:200]}...")

            logger.info("=== ê²€ìƒ‰ ê²°ê³¼ ì¢…ë£Œ ===\n")
            
            # ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                callback_handler = StreamlitCallbackHandler(message_placeholder)
                callback_manager = CallbackManager([callback_handler])
                
                # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì„ì‹œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_llm = ChatOpenAI(
                    model_name=CONFIG["model_name"],
                    temperature=0,
                    streaming=True,
                    openai_api_key=CONFIG["api_keys"]["openai"],
                    callback_manager=callback_manager
                )
                
                # ì„ì‹œ QA ì²´ì¸ ìƒì„±
                temp_qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=temp_llm,
                    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
                    memory=st.session_state['memory'],
                    return_source_documents=True,
                    chain_type="stuff",
                    combine_docs_chain_kwargs={
                        "prompt": chat_prompt,
                        "document_variable_name": "context",
                    },
                    verbose=False,
                    output_key='answer'                
                )
                
                response = temp_qa_chain({"question": question})
                
                # ì‘ë‹µì„ ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": response['answer']})

            logger.info("=== ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ ===\n")

            # ì‘ë‹µ ìƒì„± í›„ ìŠ¤í¬ë¡¤
            st.markdown("""
                <script>
                    setTimeout(scrollToBottom, 100);
                    setTimeout(scrollToBottom, 500);
                </script>
            """, unsafe_allow_html=True)

        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {question}: {str(e)}")
            st.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()  # main() í•œ ë²ˆë§Œ í˜¸ì¶œ